{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"BERT-Hierarchical.ipynb","provenance":[{"file_id":"1fzP5iu5gM_bXJUHe8w3rtqXAQsBa58K7","timestamp":1618051790897}],"collapsed_sections":[],"toc_visible":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"8cfe9b67647e4e8ab985a8ee5efd0623":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_46ca7eea05e84644983ab11328adf749","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0b9f5451ea8e4df7af270249d6019800","IPY_MODEL_ab0318b6a90e42e69087c69bb98c2940"]}},"46ca7eea05e84644983ab11328adf749":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b9f5451ea8e4df7af270249d6019800":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a4fa8b20fee423bb36e354276bb2e39","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9cd0fb6a00c7481c9234ed26e27b1217"}},"ab0318b6a90e42e69087c69bb98c2940":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5fe01c26e2ba44358d32109299be2fb6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 3.07MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_725c39359cd641bea707c6316023f460"}},"6a4fa8b20fee423bb36e354276bb2e39":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"9cd0fb6a00c7481c9234ed26e27b1217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5fe01c26e2ba44358d32109299be2fb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"725c39359cd641bea707c6316023f460":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5950bacd59c4d40bb44438b4d111a7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b48fcb44a6cc44abb45376b730fff5ef","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_72fe9a51091d47898c7961c10cc7e307","IPY_MODEL_9d52aeaa26b14b5aa0d6af6e8db70635"]}},"b48fcb44a6cc44abb45376b730fff5ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"72fe9a51091d47898c7961c10cc7e307":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_06cd49f2fcad477b8f26f2c6dc354059","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ba28ea1ee29e4b6396125e00bf1ebd1f"}},"9d52aeaa26b14b5aa0d6af6e8db70635":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_43e566e5352f42889c80a69a7101b0fb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:21&lt;00:00, 20.5B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9be41f51e411440d8059a26340828e0e"}},"06cd49f2fcad477b8f26f2c6dc354059":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ba28ea1ee29e4b6396125e00bf1ebd1f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"43e566e5352f42889c80a69a7101b0fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9be41f51e411440d8059a26340828e0e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6228a6e3bfe843ac9f1f9199571e47cb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_2b78dbf701864c78ac82fd6533bff97d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_9898d5b81b4d4dbaaa1f83b592389fa7","IPY_MODEL_2ea209d4f1a049409915572063b6e8a7"]}},"2b78dbf701864c78ac82fd6533bff97d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9898d5b81b4d4dbaaa1f83b592389fa7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_32ea9dacf0f8496e81e86a612997b7b4","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5f814d9383e1477c912b1a397ba7985a"}},"2ea209d4f1a049409915572063b6e8a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_30f061312d97483ba6cb3e73e9291fdb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:08&lt;00:00, 54.1MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_66314a0dafb74086a27d43820f01407b"}},"32ea9dacf0f8496e81e86a612997b7b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5f814d9383e1477c912b1a397ba7985a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30f061312d97483ba6cb3e73e9291fdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"66314a0dafb74086a27d43820f01407b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"LWTbkajDNwHC"},"source":["# Fine-tuning BERT on long texts"]},{"cell_type":"markdown","metadata":{"id":"QUfmeQfRmoja"},"source":["## Hierarchical Method"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EIHH1OSROtq3","executionInfo":{"status":"ok","timestamp":1618065121813,"user_tz":-330,"elapsed":22539,"user":{"displayName":"Omanshu Mahawar","photoUrl":"","userId":"07418341854440785887"}},"outputId":"d92ee12a-40c3-4233-ee61-249d1e5d947b"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tTO6xDxDR0pf"},"source":["!pip install transformers==2.10.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0PYVxi0nNwHQ","executionInfo":{"status":"ok","timestamp":1618065156272,"user_tz":-330,"elapsed":8244,"user":{"displayName":"Omanshu Mahawar","photoUrl":"","userId":"07418341854440785887"}}},"source":["import sys\n","sys.path.append(\"./drive/MyDrive/ML-Project/RoBERT\")\n","\n","%matplotlib inline\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline, FeatureUnion\n","from transformers import BertTokenizer\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import transformers\n","from transformers import RobertaTokenizer, BertTokenizer, RobertaModel, BertModel, AdamW# get_linear_schedule_with_warmup\n","from transformers import get_linear_schedule_with_warmup\n","import time\n","\n","from utils import *\n","from Custom_Dataset_Class import ConsumerComplaintsDataset1\n","from Bert_Classification import Bert_Classification_Model\n","from RoBERT import RoBERT_Model\n","\n","from BERT_Hierarchical import BERT_Hierarchical_Model\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gecJQzpZNwHS","executionInfo":{"status":"ok","timestamp":1618065157932,"user_tz":-330,"elapsed":1646,"user":{"displayName":"Omanshu Mahawar","photoUrl":"","userId":"07418341854440785887"}},"outputId":"9ee7f7b2-de69-4c45-d729-ec6cb09119e4"},"source":["# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pmZuzU7xm3sY"},"source":["### Mean Pooling"]},{"cell_type":"code","metadata":{"id":"PmqKHYw4Z_0D","colab":{"base_uri":"https://localhost:8080/","height":504,"referenced_widgets":["8cfe9b67647e4e8ab985a8ee5efd0623","46ca7eea05e84644983ab11328adf749","0b9f5451ea8e4df7af270249d6019800","ab0318b6a90e42e69087c69bb98c2940","6a4fa8b20fee423bb36e354276bb2e39","9cd0fb6a00c7481c9234ed26e27b1217","5fe01c26e2ba44358d32109299be2fb6","725c39359cd641bea707c6316023f460","d5950bacd59c4d40bb44438b4d111a7f","b48fcb44a6cc44abb45376b730fff5ef","72fe9a51091d47898c7961c10cc7e307","9d52aeaa26b14b5aa0d6af6e8db70635","06cd49f2fcad477b8f26f2c6dc354059","ba28ea1ee29e4b6396125e00bf1ebd1f","43e566e5352f42889c80a69a7101b0fb","9be41f51e411440d8059a26340828e0e","6228a6e3bfe843ac9f1f9199571e47cb","2b78dbf701864c78ac82fd6533bff97d","9898d5b81b4d4dbaaa1f83b592389fa7","2ea209d4f1a049409915572063b6e8a7","32ea9dacf0f8496e81e86a612997b7b4","5f814d9383e1477c912b1a397ba7985a","30f061312d97483ba6cb3e73e9291fdb","66314a0dafb74086a27d43820f01407b"]},"executionInfo":{"status":"ok","timestamp":1618067393181,"user_tz":-330,"elapsed":10686,"user":{"displayName":"Omanshu Mahawar","photoUrl":"","userId":"07418341854440785887"}},"outputId":"e71956fe-3f2c-4dbd-9ab1-7dfdb20ab1fa"},"source":["TRAIN_BATCH_SIZE=3\n","EPOCH=1\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","MIN_LEN=249\n","MAX_LEN = 100000\n","CHUNK_LEN=200\n","OVERLAP_LEN=50\n","#MAX_LEN=10000000\n","#MAX_SIZE_DATASET=1000\n","\n","print('Loading BERT tokenizer...')\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","dataset=ConsumerComplaintsDataset1(\n","    tokenizer=bert_tokenizer,\n","    min_len=MIN_LEN,\n","    max_len=MAX_LEN,\n","    chunk_len=CHUNK_LEN,\n","    #max_size_dataset=MAX_SIZE_DATASET,\n","    overlap_len=OVERLAP_LEN)\n","\n","\n","#train_size = int(0.8 * len(dataset))\n","#test_size = len(dataset) - train_size\n","#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_data_loader=DataLoader(\n","    dataset,\n","    batch_size=TRAIN_BATCH_SIZE,\n","    sampler=train_sampler,\n","    collate_fn=my_collate1)\n","\n","valid_data_loader=DataLoader(\n","    dataset,\n","    batch_size=TRAIN_BATCH_SIZE,\n","    sampler=valid_sampler,\n","    collate_fn=my_collate1)\n","\n","\n","device=torch.device(\"cuda\")\n","lr=3e-5#1e-3\n","num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n","\n","pooling_method=\"mean\"\n","model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n","optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                        num_warmup_steps = 0,\n","                                        num_training_steps = num_training_steps)\n","val_losses=[]\n","batches_losses=[]\n","val_acc=[]\n","for epoch in range(EPOCH):\n","    t0 = time.time()    \n","    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n","    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n","    epoch_loss=np.mean(batches_losses_tmp)\n","    print(f\"\\nAvg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec)\\n\")\n","    t1=time.time()\n","    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n","    print(f\"==> Evaluation : Avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n","    tmp_evaluate=evaluate(target.reshape(-1), output)\n","    print(f\"=====>\\t{tmp_evaluate}\")\n","    val_acc.append(tmp_evaluate['accuracy'])\n","    val_losses.append(val_losses_tmp)\n","    batches_losses.append(batches_losses_tmp)\n","    print(f\"\\The Hierarchical {pooling_method} pooling model has been saved in the drive\")\n","    torch.save(model_hierarchical, f\"./drive/MyDrive/ML-Project/RoBERT/model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")    "],"execution_count":5,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8cfe9b67647e4e8ab985a8ee5efd0623","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Data cleaning\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d5950bacd59c4d40bb44438b4d111a7f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6228a6e3bfe843ac9f1f9199571e47cb","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","\n","=============== EPOCH 1 / 1 ===============\n","\n","___ batch index = 0 / 4572 (0.00%), loss = 2.8737, time = 0.85 secondes ___\n","___ batch index = 640 / 4572 (14.00%), loss = 0.4090, time = 247.83 secondes ___\n","___ batch index = 1280 / 4572 (28.00%), loss = 0.4200, time = 262.88 secondes ___\n","___ batch index = 1920 / 4572 (41.99%), loss = 0.4946, time = 259.18 secondes ___\n","___ batch index = 2560 / 4572 (55.99%), loss = 0.6548, time = 258.58 secondes ___\n","___ batch index = 3200 / 4572 (69.99%), loss = 0.3331, time = 256.04 secondes ___\n","___ batch index = 3840 / 4572 (83.99%), loss = 0.5276, time = 260.84 secondes ___\n","___ batch index = 4480 / 4572 (97.99%), loss = 0.4688, time = 262.56 secondes ___\n","\n","Avg_loss : 0.64, time : ~30.0 min (1845.29 sec)\n","\n","==> Evaluation : Avg_loss = 0.46, time : 162.38 sec\n","\n","=====>\t{'accuracy': 0.8684364060676779, 'nb exemple': 3428, 'true_prediction': 2977, 'false_prediction': 451}\n","\\The Hierarchical mean pooling model has been saved in the drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PZyRUBX5nO1k"},"source":["### Max Pooling"]},{"cell_type":"code","metadata":{"id":"V5gVZ48knQFH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618070999833,"user_tz":-330,"elapsed":2031808,"user":{"displayName":"Omanshu Mahawar","photoUrl":"","userId":"07418341854440785887"}},"outputId":"b7162282-3eba-4059-ff69-7bffe1bbdcaf"},"source":["TRAIN_BATCH_SIZE=3\n","EPOCH=1\n","validation_split = .2\n","shuffle_dataset = True\n","random_seed= 42\n","MIN_LEN=249\n","MAX_LEN = 100000\n","CHUNK_LEN=200\n","OVERLAP_LEN=50\n","#MAX_LEN=10000000\n","#MAX_SIZE_DATASET=1000\n","\n","print('Loading BERT tokenizer...')\n","bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","dataset=ConsumerComplaintsDataset1(\n","    tokenizer=bert_tokenizer,\n","    min_len=MIN_LEN,\n","    max_len=MAX_LEN,\n","    chunk_len=CHUNK_LEN,\n","    #max_size_dataset=MAX_SIZE_DATASET,\n","    overlap_len=OVERLAP_LEN)\n","\n","\n","#train_size = int(0.8 * len(dataset))\n","#test_size = len(dataset) - train_size\n","#train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","# Creating data indices for training and validation splits:\n","dataset_size = len(dataset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","if shuffle_dataset :\n","    np.random.seed(random_seed)\n","    np.random.shuffle(indices)\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers and loaders:\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_data_loader=DataLoader(\n","    dataset,\n","    batch_size=TRAIN_BATCH_SIZE,\n","    sampler=train_sampler,\n","    collate_fn=my_collate1)\n","\n","valid_data_loader=DataLoader(\n","    dataset,\n","    batch_size=TRAIN_BATCH_SIZE,\n","    sampler=valid_sampler,\n","    collate_fn=my_collate1)\n","\n","\n","device=torch.device(\"cuda\")\n","lr=3e-5#1e-3\n","num_training_steps=int(len(dataset) / TRAIN_BATCH_SIZE * EPOCH)\n","\n","pooling_method=\"max\"\n","model_hierarchical=BERT_Hierarchical_Model(pooling_method=pooling_method).to(device)\n","optimizer=AdamW(model_hierarchical.parameters(), lr=lr)\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                        num_warmup_steps = 0,\n","                                        num_training_steps = num_training_steps)\n","val_losses=[]\n","batches_losses=[]\n","val_acc=[]\n","for epoch in range(EPOCH):\n","    t0 = time.time()    \n","    print(f\"\\n=============== EPOCH {epoch+1} / {EPOCH} ===============\\n\")\n","    batches_losses_tmp=rnn_train_loop_fun1(train_data_loader, model_hierarchical, optimizer, device)\n","    epoch_loss=np.mean(batches_losses_tmp)\n","    print(f\"\\n*** avg_loss : {epoch_loss:.2f}, time : ~{(time.time()-t0)//60} min ({time.time()-t0:.2f} sec) ***\\n\")\n","    t1=time.time()\n","    output, target, val_losses_tmp=rnn_eval_loop_fun1(valid_data_loader, model_hierarchical, device)\n","    print(f\"==> evaluation : avg_loss = {np.mean(val_losses_tmp):.2f}, time : {time.time()-t1:.2f} sec\\n\")    \n","    tmp_evaluate=evaluate(target.reshape(-1), output)\n","    print(f\"=====>\\t{tmp_evaluate}\")\n","    val_acc.append(tmp_evaluate['accuracy'])\n","    val_losses.append(val_losses_tmp)\n","    batches_losses.append(batches_losses_tmp)\n","    print(f\"\\t The Hierarchical {pooling_method} pooling model has been saved in the drive\")\n","    torch.save(model_hierarchical, f\"./drive/MyDrive/ML-Project/RoBERT/model_hierarchical/{pooling_method}_pooling/model_{pooling_method}_pooling_epoch{epoch+1}.pt\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Loading BERT tokenizer...\n","Data cleaning\n","\n","=============== EPOCH 1 / 1 ===============\n","\n","___ batch index = 0 / 4572 (0.00%), loss = 2.3443, time = 0.47 secondes ___\n","___ batch index = 640 / 4572 (14.00%), loss = 0.7449, time = 258.16 secondes ___\n","___ batch index = 1280 / 4572 (28.00%), loss = 0.7959, time = 259.83 secondes ___\n","___ batch index = 1920 / 4572 (41.99%), loss = 0.4115, time = 260.40 secondes ___\n","___ batch index = 2560 / 4572 (55.99%), loss = 0.3908, time = 262.10 secondes ___\n","___ batch index = 3200 / 4572 (69.99%), loss = 0.1945, time = 262.26 secondes ___\n","___ batch index = 3840 / 4572 (83.99%), loss = 0.3627, time = 259.11 secondes ___\n","___ batch index = 4480 / 4572 (97.99%), loss = 0.3941, time = 259.55 secondes ___\n","\n","*** avg_loss : 0.59, time : ~30.0 min (1858.37 sec) ***\n","\n","==> evaluation : avg_loss = 0.43, time : 161.29 sec\n","\n","=====>\t{'accuracy': 0.8722287047841307, 'nb exemple': 3428, 'true_prediction': 2990, 'false_prediction': 438}\n","\t The Hierarchical max pooling model has been saved in the drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pBJSJGw1Z4tg"},"source":[""],"execution_count":null,"outputs":[]}]}